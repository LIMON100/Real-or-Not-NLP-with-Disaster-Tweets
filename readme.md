# Real-or-Not-NLP-with-Disaster-Tweets



## Problem Description:

Twitter has become an important communication channel in times of emergency.The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).
But, it’s not always clear whether a person’s words are actually announcing a disaster. By solving fake tweets problem, we build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.





## Dataset: 

https://www.kaggle.com/c/nlp-getting-started/data



## Working Process:

This is mainly a natural language processing problem. Besides ML algorithm there are many NLP tecniques for prepare the best data for a better result. We try to apply many of them.


### EDA

- Visualize disaster and non-disaster tweets 

- Character difference in both

- Words difference in both

- Average word length

- Tweets per location

- Visualize the location map

- Common words in tweets

- Number of punctuations






## Data Preproceessing


1 . Removing stop words(Optional)

2 . Remove Punctuations

3 . Remove Html

4 . Remove Emojis

5 . Spelling Corrections

6 . Removing Urls




## Conveting Text

In NLP there are many tecniques to convert text.Those are,

- **BOW**

The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity


- **W2V**

Word2vec is a technique for natural language processing. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.


- **Bi-Gram**

A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.


- **Response-Encoding**

The response encoding is the character encoding of the textual response generated by a web component. The response encoding must be set appropriately so that the characters are rendered correctly for a given locale.


- **TFIDF**

In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.


- **TFIDF-W2V**

It's a combine tecnique of tfidf and w2v.




## Apply every ml algorithm in above tecniques.The algorithms are,

**Logistic Regression**

**KNN**

**SVM**


**DT**

**Random-Forest**

**Gradient-Boosting**

**XGBoost**
